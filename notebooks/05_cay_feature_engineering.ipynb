{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5365ab",
   "metadata": {},
   "source": [
    "# TL;DR\n",
    "Removing stopwords obviously allows to reduce the number of tokens per narratives. The percentage of narratives whose lenths > to max_length have been divided by 3-4. We tried to filter words whose tf-idf are in stopwords' tf-idf range. But this method is not scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db842eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import tiktoken\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/01_primary/one_hot_mapping.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "with open(\"../data/01_primary/abs_decoder_domain_specific.json\", \"r\") as f:\n",
    "    decoder = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_train = pd.read_parquet(\"../data/01_primary/asrs_data_primary_train.parquet\")\n",
    "df1b_trained_filtered = df1b_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afeee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "berttokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tiktokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f1448",
   "metadata": {},
   "source": [
    "# Count tokens per narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a8d2e",
   "metadata": {},
   "source": [
    "## W/ berttokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_count = [\n",
    "    len(berttokenizer.encode(narrative)) for narrative in df1b_train.narrative.tolist()\n",
    "]\n",
    "sns.histplot(bert_count, bins=200, stat=\"percent\")\n",
    "plt.axvline(x=512, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    512 + 30,\n",
    "    plt.ylim()[1] * 0.7,\n",
    "    f\"max length = 512\",\n",
    "    color=\"red\",\n",
    "    rotation=90,\n",
    "    fontdict={\"fontsize\": 8},\n",
    ")\n",
    "plt.text(\n",
    "    700,\n",
    "    plt.ylim()[1] * 0.5,\n",
    "    f\"{(len(np.where(np.array(bert_count)>512)[0])/len(bert_count)) * 100:.2f}% > max length\",\n",
    "    color=\"blue\",\n",
    "    fontdict={\"fontsize\": 12},\n",
    ")\n",
    "plt.title(\"Train set narratives lengths distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070d45a",
   "metadata": {},
   "source": [
    "## W/ tiktokenier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885597d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_count = [\n",
    "    len(tiktokenizer.encode(narrative)) for narrative in df1b_train.narrative.tolist()\n",
    "]\n",
    "sns.histplot(tik_count, bins=200, stat=\"percent\")\n",
    "plt.axvline(x=512, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    512 + 30,\n",
    "    plt.ylim()[1] * 0.7,\n",
    "    f\"max length = 512\",\n",
    "    color=\"red\",\n",
    "    rotation=90,\n",
    "    fontdict={\"fontsize\": 8},\n",
    ")\n",
    "plt.text(\n",
    "    700,\n",
    "    plt.ylim()[1] * 0.5,\n",
    "    f\"{(len(np.where(np.array(tik_count)>512)[0])/len(tik_count)) * 100:.2f}% > max length\",\n",
    "    color=\"blue\",\n",
    "    fontdict={\"fontsize\": 12},\n",
    ")\n",
    "plt.title(\"Train set narratives lengths distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0542a",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eed60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: \"str\", lang: \"spacy.Language\"):\n",
    "    pattern = re.compile(r\"\\s+([.,!?;:])\")\n",
    "    docs = nlp(text)\n",
    "    filtered = \" \".join([token.text for token in docs if not token.is_stop])\n",
    "    filtered = pattern.sub(r\"\\1\", filtered)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e05ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_trained_filtered.loc[[True] * len(df1b_trained_filtered), \"narrative\"] = (\n",
    "    df1b_trained_filtered.narrative.apply(lambda t: remove_stopwords(text=t, lang=nlp))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_count = [\n",
    "    len(berttokenizer.encode(narrative))\n",
    "    for narrative in df1b_trained_filtered.narrative.tolist()\n",
    "]\n",
    "sns.histplot(bert_count, bins=200, stat=\"percent\")\n",
    "plt.axvline(x=512, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    512 + 30,\n",
    "    plt.ylim()[1] * 0.7,\n",
    "    f\"max length = 512\",\n",
    "    color=\"red\",\n",
    "    rotation=90,\n",
    "    fontdict={\"fontsize\": 8},\n",
    ")\n",
    "plt.text(\n",
    "    700,\n",
    "    plt.ylim()[1] * 0.5,\n",
    "    f\"{(len(np.where(np.array(bert_count)>512)[0])/len(bert_count)) * 100:.2f}% > max length\",\n",
    "    color=\"blue\",\n",
    "    fontdict={\"fontsize\": 12},\n",
    ")\n",
    "plt.title(\"Train set narratives W/o stopwords lengths distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "tik_count = [\n",
    "    len(tiktokenizer.encode(narrative))\n",
    "    for narrative in df1b_trained_filtered.narrative.tolist()\n",
    "]\n",
    "sns.histplot(tik_count, bins=200, stat=\"percent\")\n",
    "plt.axvline(x=512, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    512 + 30,\n",
    "    plt.ylim()[1] * 0.7,\n",
    "    f\"max length = 512\",\n",
    "    color=\"red\",\n",
    "    rotation=90,\n",
    "    fontdict={\"fontsize\": 8},\n",
    ")\n",
    "plt.text(\n",
    "    700,\n",
    "    plt.ylim()[1] * 0.5,\n",
    "    f\"{(len(np.where(np.array(tik_count)>512)[0])/len(tik_count)) * 100:.2f}% > max length\",\n",
    "    color=\"blue\",\n",
    "    fontdict={\"fontsize\": 12},\n",
    ")\n",
    "plt.title(\"Train set narratives W/o stopwords lengths distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823f2ed",
   "metadata": {},
   "source": [
    "# CLEAN OUT FREQUENT WORDS, DECODE ABBS AND CLEAN USELESS PUNCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1590c50",
   "metadata": {},
   "source": [
    "## Compute TFIDF for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(dtype=np.float32, lowercase=False, norm=\"l1\")\n",
    "\n",
    "df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vectorizer.fit_transform(raw_documents=df1b_trained_filtered.narrative.tolist()),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96055674",
   "metadata": {},
   "source": [
    "##  decode abbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96978685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_pattern(decoder: dict) -> re.Pattern:\n",
    "    terms = set()\n",
    "    for key in decoder:\n",
    "        terms.update({key, key.lower(), key.capitalize()} if key.isupper() else {key})\n",
    "    escaped_terms = [r\"(?<!\\w)\" + re.escape(term) + r\"(?!\\w)\" for term in terms]\n",
    "    return re.compile(r\"(\" + \"|\".join(escaped_terms) + r\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b882c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_abs(text: \"str\", pattern: \"re.Pattern\", decoder: \"dict[str, str]\"):\n",
    "    matched_abbs = [\n",
    "        (abb.upper() if (abb.istitle() or abb.islower()) else abb)\n",
    "        for abb in set(pattern.findall(text.strip().replace(\" / \", \"/\")))\n",
    "    ]\n",
    "    for abb in matched_abbs:\n",
    "        text = text.replace(abb, decoder[abb])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81494638",
   "metadata": {},
   "source": [
    "## Clean punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punc(text: \"str\"):\n",
    "    t = re.sub(\n",
    "        r\"([a-zA-Z0-9])\\s+([.,!?;:])\", r\"\\1\\2\", text\n",
    "    )  # Coller la ponctuation au mot précédent\n",
    "    t = text = re.sub(\n",
    "        r\"(\\w)([!?])\\2+\", r\"\\1\\2\", t\n",
    "    )  # Réduire les ponctuations répétées identiques (!!, ??) à une seule\n",
    "    t = re.sub(\n",
    "        r\"(\\w)[!?]{2,}\", r\"\\1?\", t\n",
    "    )  # Remplacer les séquences mixtes de !? ou ?! ou !?!? etc. par ?\n",
    "    t = re.sub(\n",
    "        r\"([.,;!?])[.,;!?]{2,}\", r\"\\1\", t\n",
    "    )  # Supprimer les groupes de ponctuation trop longs (ex: \"!!!\", \",,,\", etc.)\n",
    "    # Supprimer les ponctuations orphelines (début ou fin ou entre espaces)\n",
    "    t = re.sub(r\"(^|\\s)[.,;!?](?=\\s|$)\", r\"\\1\", t)\n",
    "    t = re.sub(r\"^[.,;!?]\\s+\", \"\", t)\n",
    "    # Nettoyer les espaces multiples\n",
    "    clned_text = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    return clned_text\n",
    "\n",
    "\n",
    "clean_punc(\n",
    "    \"Bonjour , comment ça va ? Très bien , merci ! ;,, . Bonjour ; ensuite. Incroyable!! Quoi?? C'est fou!!?? Non?! ok. Bonjour ?!;;!! ok. Bonjour?!;;!!\"\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5a47c",
   "metadata": {},
   "source": [
    "## Compute filterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd70508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_narrative(\n",
    "    i: \"int\",\n",
    "    text: \"str\",\n",
    "    lang: \"spacy.Language\",\n",
    "    tdidf: \"pd.DataFrame\",\n",
    "    code_patterns: \"re.Pattern\",\n",
    "    decoder: \"dict[str, str]\",\n",
    "    thd: \"float\",\n",
    "    decode: \"bool\" = True,\n",
    "):\n",
    "    tdidf_row = tdidf.iloc[i]\n",
    "    words = tdidf_row[tdidf_row > thd].index\n",
    "    docs = lang(text)\n",
    "    tokens = []\n",
    "    for token in docs:\n",
    "        if token.is_punct:\n",
    "            tokens.append(token.text)\n",
    "        else:\n",
    "            if token.text in words:\n",
    "                tokens.append(\n",
    "                    decode_abs(text=token.text, pattern=code_patterns, decoder=decoder)\n",
    "                    if decode\n",
    "                    else token.text\n",
    "                )\n",
    "    return clean_punc(\" \".join(tokens)).lower()\n",
    "\n",
    "\n",
    "def filter_narratives(\n",
    "    narratives: \"list[str]\",\n",
    "    lang: \"spacy.Language\",\n",
    "    tdidf: \"pd.DataFrame\",\n",
    "    code_patterns: \"re.Pattern\",\n",
    "    decoder: \"dict[str, str]\",\n",
    "    thd: \"float\" = 0.0,\n",
    "    decode: \"bool\" = True,\n",
    "    max_workers: \"int | None\" = None,\n",
    "):\n",
    "    func = partial(\n",
    "        process_single_narrative,\n",
    "        tdidf=tdidf,\n",
    "        lang=lang,\n",
    "        code_patterns=code_patterns,\n",
    "        decoder=decoder,\n",
    "        thd=thd,\n",
    "        decode=decode,\n",
    "    )\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                executor.map(lambda args: func(*args), enumerate(narratives)),\n",
    "                total=len(narratives),\n",
    "                desc=\"Filtering...\",\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = build_decoder_pattern(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893c0c9",
   "metadata": {},
   "source": [
    "## W/o Decoding ABBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "filtered_narratives = filter_narratives(\n",
    "    narratives=df1b_trained_filtered.narrative.tolist()[:100],\n",
    "    lang=nlp,\n",
    "    tdidf=df,\n",
    "    thd=1e-10,\n",
    "    code_patterns=pattern,\n",
    "    decoder=decoder,\n",
    "    decode=False,\n",
    "    max_workers=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab31195",
   "metadata": {},
   "source": [
    "it takes 5 minutes to process 100 narratives. not sclable to 100k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43ea6e",
   "metadata": {},
   "source": [
    "## Decoding abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = partial(decode_abs, pattern=pattern, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_narratives_wo_stopwords = [\n",
    "    clean_punc(func(text)) for text in df1b_trained_filtered.narrative.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bef0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_count = [\n",
    "    len(berttokenizer.encode(narrative)) for narrative in dec_narratives_wo_stopwords\n",
    "]\n",
    "sns.histplot(bert_count, bins=200, stat=\"percent\")\n",
    "plt.axvline(x=512, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    512 + 30,\n",
    "    plt.ylim()[1] * 0.7,\n",
    "    f\"max length = 512\",\n",
    "    color=\"red\",\n",
    "    rotation=90,\n",
    "    fontdict={\"fontsize\": 8},\n",
    ")\n",
    "plt.text(\n",
    "    700,\n",
    "    plt.ylim()[1] * 0.5,\n",
    "    f\"{(len(np.where(np.array(bert_count)>512)[0])/len(bert_count)) * 100:.2f}% > max length\",\n",
    "    color=\"blue\",\n",
    "    fontdict={\"fontsize\": 12},\n",
    ")\n",
    "plt.title(\"Decoded Train set narratives lengths distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4164a4",
   "metadata": {},
   "source": [
    "# ONE HOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(text: \"str\", mapping: \"dict[str, int]\"):\n",
    "    num_labels = [0] * len(mapping)\n",
    "    labels = [label.strip().replace(\" / \", \"/\") for label in text.split(\";\")]\n",
    "    for label in labels:\n",
    "        num_labels[mapping[label]] = 1\n",
    "    return num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_trained_filtered[[True] * len(df1b_trained_filtered), \"anomaly\"] = (\n",
    "    df1b_trained_filtered.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_trained_filtered.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_train_stopwords.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b15ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_train_decoded = df1b_trained_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_train_decoded[\"narrative\"] = dec_narratives_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_train_decoded.loc[[True] * len(df1b_train_decoded), \"anomaly\"] = (\n",
    "    df1b_train_decoded.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83987b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_train_decoded.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_train_decoded.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b8c2d",
   "metadata": {},
   "source": [
    "# PROCESSING VALIDATION AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation = pd.read_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_validation.parquet\"\n",
    ")\n",
    "df1b_test = pd.read_parquet(\"../data/01_primary/asrs_data_primary_test.parquet\")\n",
    "df1b_validation_filtered = df1b_validation.copy()\n",
    "df1b_test_filtered = df1b_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e774ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_filtered.loc[[True] * len(df1b_validation_filtered), \"narrative\"] = (\n",
    "    df1b_validation_filtered.narrative.apply(\n",
    "        lambda t: remove_stopwords(text=t, lang=nlp)\n",
    "    )\n",
    ")\n",
    "\n",
    "df1b_test_filtered.loc[[True] * len(df1b_test_filtered), \"narrative\"] = (\n",
    "    df1b_test_filtered.narrative.apply(lambda t: remove_stopwords(text=t, lang=nlp))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a98a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dec_narratives_wo_stopwords = [\n",
    "    clean_punc(func(text)) for text in df1b_validation_filtered.narrative.tolist()\n",
    "]\n",
    "test_dec_narratives_wo_stopwords = [\n",
    "    clean_punc(func(text)) for text in df1b_test_filtered.narrative.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_filtered[[True] * len(df1b_validation_filtered), \"anomaly\"] = (\n",
    "    df1b_validation_filtered.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")\n",
    "df1b_test_filtered[[True] * len(df1b_test_filtered), \"anomaly\"] = (\n",
    "    df1b_test_filtered.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fce17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_filtered.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_validation_stopwords.parquet\"\n",
    ")\n",
    "df1b_test_filtered.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_test_stopwords.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38be89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_decoded = df1b_validation_filtered.copy()\n",
    "df1b_test_decoded = df1b_test_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_decoded[\"narrative\"] = val_dec_narratives_wo_stopwords\n",
    "df1b_test_decoded[\"narrative\"] = test_dec_narratives_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a331b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b_validation_decoded.loc[[True] * len(df1b_validation_decoded), \"anomaly\"] = (\n",
    "    df1b_validation_decoded.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")\n",
    "df1b_test_decoded.loc[[True] * len(df1b_test_decoded), \"anomaly\"] = (\n",
    "    df1b_test_decoded.anomaly.apply(lambda t: one_hot(text=t, mapping=mapping))\n",
    ")\n",
    "df1b_validation_decoded.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_validation_decoded.parquet\"\n",
    ")\n",
    "df1b_test_decoded.to_parquet(\n",
    "    \"../data/01_primary/asrs_data_primary_test_decoded.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
